---
title: "3. DN - Bayesova Statistika: Primer hierarhicnega modela z Gibbsovim vzorcevalnikom"
author: "Jan Črne"
fontsize: 12pt
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 5, out.width = "0.8\\textwidth")
```

Hierarhicni model iz 6. sklopa posplosimo tako, da dovolimo razlicne variance skupin $\sigma_j^2$, ki so neodvisno enako porazdeljene z inverzno gama porazdelitvijo s parametri, za katere dolocimo neko hiperapriorno porazdelitev. Natancneje:

Variabilnost znotraj skupine (*within-group sampling variability*):

$$
(X_{1, j}, \dots, X_{n_{j}, j}) \mid \mu_j, \sigma_j^2 \sim \text{n.e.p.}\; N(\mu_j, \sigma_j^2)
$$

Variabilnost med skupinami (*between-group sampling variability*):
$$
(\mu_{1}, \dots, \mu_{m}) \mid \mu, \eta^2 \sim \text{n.e.p.} \; N(\mu, \eta^2), \quad \text{tj. enako kot prej};
$$

$$
(\sigma^2_{1}, \dots, \sigma^2_{m}) \mid \sigma_0^2,\nu_0 \sim \text{n.e.p.} \; \text{Inv-Gama}(\nu_0 / 2, \sigma_0^2 \nu_0 / 2).
$$

Hiperapriorne porazdelitve $\mu, \eta^2, \sigma_0^2, \nu_0$ naj bodo neodvisne. Hiperapriorna porazdelitev $\mu$ in $\eta^2$ naj bo enaka kot prej. Hiperapriorna porazdelitev za $\sigma_0^2$ naj bo:
$$
\sigma^2_0 \sim \text{Gama}(a,b), \quad \text{kjer vzamemo } a=2, b=1/10;
$$

Hiperapriorna porazdelitev za $\nu_0$ naj bo diskretna z vrednostmi $k \in \{1,2,\ldots\}$, za katero velja $P(\nu_0=k) \propto e^{-\alpha k}$, kjer vzamemo $\alpha=2$.

Za uporabo Gibbsovega vzorcevalnika potrebujemo pogojne porazdelitve (hiper)parametrov. Hiperparmetra $\mu$ in $\eta^2$ imata enako pogojno porazdelitev kot prej. Pri pogojni porazdelitvi za $\mu_j$ moramo v prejsnji formuli nadomestiti $\sigma^2$ z $\sigma_j^2$, tj.

$$
\mu_j \mid \text{vse ostalo} \sim N\left( \frac{\bar{x}_{\cdot j}n_j / \sigma_j^2 + \mu / \eta^2}{n_j / \sigma_j^2 + 1 / \eta^2}, \left[ n_j / \sigma_j^2 + 1 / \eta^2 \right]^{-1} \right).
$$



Z uporabo izpeljav pri normalnem modelu lahko izpeljemo, da za vsak $j$ velja

$$
\sigma_j^2 \mid \text{vse ostalo} \sim \text{Inv-Gama}\left( \frac{\nu_0 + n_j}{2}, \frac{\nu_0\sigma_0^2 + \sum_{i = 1}^{n_j} (x_{i, j} - \mu_j)^2}{2} \right).
$$


Izpeljemo lahko
$$
\sigma_0^2 \mid \text{vse ostalo} \sim \text{Gama}\left(a+ \frac{m\nu_0 }{2}, b+\frac{\nu_0}{2}\sum_{j = 1}^{m} (1/\sigma^2_j) \right).
$$


Za pogojno porazdelitev $\nu_0$ lahko pokazemo, da je za vsak $k \in \{1,2,\ldots\}$
\begin{equation}
P(\nu_0=k\mid\text{vse ostalo}) \propto \left(\frac{(k\sigma_0^2/2)^{k/2}}{\Gamma(k/2)}\right)^m  \left(\prod_{j=1}^m(1/\sigma_j^2)\right)^{k/2-1}  \exp\left(-k\left(\alpha+\frac{1}{2}\sigma_o^2\sum_{j=1}^m(1/\sigma_j^2)\right)\right).
\end{equation}
Iz te porazdelitve lahko vzorcimo, ce se omejimo na velik izbor $k \in \{1,2,\ldots\,k_{\text{max}}\}$, za te $k$ izracunamo zgornji izraz in nato vzorcimo $\nu_0$ iz mnozice $\{1,2,\ldots\,k_{\text{max}}\}$ z utezmi (1). To je bolje narediti na $\log$ skali in na koncu uteži "preskalirati". V pomoc pri vzorcenju iz te porazdelitve vam je lahko naslednja koda v R: 

\textbf{Vasa naloga je}, da za opisani hierarhicni model z razlicnimi variancami po skupinah vzorcite aposteriorno porazdelitev parametrov s pomocjo Gibbsosovega vzorcevalnika. To naredite tako, da ustrezno predrugacite prejsno kodo Gibbsosovega vzorcevalnika za hierarhicni model z enakimi variancami.


# Podatki

V raziskavi *Educational Longitudinal Study (ELS)* iz leta 2002 so preucevali rezultate testov ucencev ameriskih srednjih sol. Podane imamo rezultate matematicnih testov ucencev 10. razreda iz 100 javnih srednjih sol (velike sole z vsaj 400 ucenci 10. razreda, urbano okolje).

Za vsakega ucenca imamo podano solo in rezultat matematicnega testa -- nasi podatki so torej vecnivojski/hierarhicni.

Rezultati matematicnega testa so del nacionalnega preverjanja znanja, ki naj bi bil konstruiran tako, da je pricakovana vrednost enaka 50 in standardni odklon 10.

Oglejmo si podatke.


```{r}
source("podatki_sole.R")
str(pod)
```

```{r message=FALSE, warning=FALSE}
library(dplyr)
# Preureditev podatkov:
pod.sole <- pod %>%
  group_by(school) %>%
  summarise(povprecje = mean(mathscore), 
            n=length(mathscore), 
            varianca = var(mathscore))
```

# Preurejen Gibbsov vzorčevalnik


```{r}
### Parametri (hiper)apriornih porazdelitev (enako kot prej)
sigma20 = 100
nu0 = 1
eta20 = 100
kappa0 = 1
mu0 = 50
tau20 = 25

### Pripravimo si kolicine, ki jih bomo potrebovali iz podatkov
x = pod
m = length(pod.sole$school)
n = pod.sole$n
x.povpr = pod.sole$povprecje
# NA NOVO
x.var   <- pod.sole$varianca  # variance posameznih šol, sedaj te podatke rabimo

# NA NOVO
# za variance posameznih šol
a <- 2 # parameter za  gama porazdeljeno sigma2
b <- 1/10 # parameter za  gama porazdeljeno sigma2
alpha <- 2 # parameter za diskretno porazdeljeni nu0
k.max <- 1000  # izberemo si neko dovolj veliko vrednost za verjetnost 
               # iz katere se bomo vzorčili nu0

### Dolocimo si zacetne vrednosti
muGroups = x.povpr
sigma2Groups = x.var  #sedaj za vsako šolo rabimo svojo začetnon vrednost variance
mu = mean(muGroups)
eta2 = var(muGroups)

### Pripravimo si prostor za shranjevanje
n.iter = 5000

muGroups.all = matrix(nrow = n.iter, ncol = m)
sigma2Groups.all = matrix(nrow = n.iter, ncol = m) #za shranjevanje varianc
sigma20.all = rep(NA, n.iter)
mu.all = rep(NA, n.iter)
eta2.all = rep(NA, n.iter)
nu0.all = rep(NA, n.iter) # za shranjevanje nu0
```


```{r}
### Na prvo mesto si shranimo zacetne vrednosti (nepotrebno)
muGroups.all[1, ] = muGroups
sigma2Groups.all[1] = sigma2Groups
mu.all[1] = mu
eta2.all[1] = eta2

sigma20.all[1] = sigma20 #zac. vr za sigma0
sigma2Groups.all[1,] = sigma2Groups #zac. vrednosti za sigme
nu0.all[1] = nu0 #zac. vr. za nu0

### Pozenemo Gibbsov vzorcevalnik
set.seed(1)
for (s in 1:n.iter) {
  ### Vzorcimo muGroups (sigma2 smo zamenjali s sigma2Groups[j])
  for (j in 1:m) {
    muGroups[j] = rnorm(1,
                        mean = (x.povpr[j] * n[j] / sigma2Groups[j] + mu / eta2) / 
                          (n[j] / sigma2Groups[j] + 1 / eta2),
                        sd = sqrt(1 / (n[j] / sigma2Groups[j] + 1 / eta2)))
  }
  

  # Vzorčimo sigma2_j (NOVO)
  for (j in 1:m) {
    sigma2Groups[j] <- 1 / rgamma(1, (nu0 + n[j]) / 2,
                                  (nu0*sigma20 + 
                                     sum((x[x[, 1] == j, 2] - muGroups[j])^2))/2)
  }
  
   # Vzorčimo sigma20 (hiperparameter) (gamma porazdeljen)
   sigma20 <- rgamma(1, a + m*nu0/2,b +nu0/2*sum(1/sigma2Groups))
  
  ### Vzorcimo mu
  mu = rnorm(1, mean = (mean(muGroups) * m / eta2 
                        + mu0 / tau20) / (m / eta2 + 1 /tau20), sd = sqrt(1 / (m / eta2 + 1 /tau20)))
  
  ### Vzorcimo eta2
  ss = kappa0 * eta20 + sum((muGroups - mu)^2)
  eta2 = 1 / rgamma(1, (kappa0 + m) / 2, ss / 2)
  
  # Vzorčimo nu0 (NOVO)
  k <- 1:k.max
  logp.nu0 <- m * (0.5 * k * log(k*sigma20/2) - lgamma(k/2)) +
    (k/2-1) * sum(log(1/sigma2Groups)) +
    - k * (alpha + 0.5 * sigma20 * sum(1/sigma2Groups))
  nu0 <- sample(k, 1, prob = exp(logp.nu0 - max(logp.nu0)))
  
  ### Shranimo nove parametre
  muGroups.all[s, ] = muGroups
  sigma2Groups.all[s, ] = sigma2Groups # za shranjevanje sigma_j
  mu.all[s] = mu
  eta2.all[s] = eta2
  sigma20.all[s] = sigma20 # za shranjevanje sigma0 (hiperparameter)
  nu0.all[s] = nu0 # za shranjevanje nu0
}
```


# Preucevanje konvergence

Uporabite zadostno stevilo iteracij in ustrezen *burn-in*. Za dobljeni vzorec preucite konvergenco, tako da sledite korakom v sklopu 6. (*trace plots*, porezdelitve podvzorcev, avtokorelacije, *effective sample size*.) Slednje naredite za vse hiperparametre in nekaj "ne-hiper-parametrov". Pri tem tudi komentirajte konvergenco. 

## *Trace plots*

```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}
library(ggplot2)

mu      <- cbind(mu.all,      1, 1:n.iter)
sigma20 <- cbind(sigma20.all, 2, 1:n.iter)
eta2    <- cbind(eta2.all,    3, 1:n.iter)
nu0     <- cbind(nu0.all,     4, 1:n.iter)

hiperparametri <- data.frame(rbind(mu, sigma20, eta2, nu0))
colnames(hiperparametri) <- c("value", "vars_", "x")

facet_names <- c(
    `1` = expression(mu),
    `2` = expression(sigma[0]^2),
    `3` = expression(eta^2),
    `4` = expression(nu[0])
)

hiperparametri <- as_tibble(hiperparametri %>% mutate_at(vars(c("value", "x")), as.double))

hiperparametri <- mutate_at(hiperparametri, 
                        .vars="vars_", 
                        .funs=factor,
                        labels = facet_names)

ggplot(hiperparametri, aes(x=x, y=value)) + 
    geom_line() + 
    facet_wrap(~vars_,
               scales="free_y",
               ncol=1,
               labeller=label_parsed) +
    labs(x="st. vzorcenj",
         y="vrednost parametra",
         title="Trace ploti za hiperparametre")
    
```


```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}
mu      <- cbind(mu.all[1:500],      1, 1:500)
sigma20 <- cbind(sigma20.all[1:500], 2, 1:500)
eta2    <- cbind(eta2.all[1:500],    3, 1:500)
nu0     <- cbind(nu0.all[1:500],     4, 1:500)

hiperparametri500 <- data.frame(rbind(mu, sigma20, eta2, nu0))
colnames(hiperparametri500) <- c("value", "vars_", "x")

facet_names <- c(
    `1` = expression(mu),
    `2` = expression(sigma[0]^2),
    `3` = expression(eta^2),
    `4` = expression(nu[0])
)

hiperparametri500 <- as_tibble(hiperparametri500 %>% mutate_at(vars(c("value", "x")), as.double))

hiperparametri500 <- mutate_at(hiperparametri500, 
                        .vars="vars_", 
                        .funs=factor,
                        labels=facet_names)

ggplot(hiperparametri500, aes(x=x, y=value)) + 
    geom_line() + 
    facet_wrap(~vars_,
               scales="free_y",
               ncol=2,
               labeller=label_parsed) +
    labs(x="st. vzorcenj",
         y="vrednost parametra",
         title="Trace ploti za hiperparametre (prvih 500 členov)")
```

Konvergence izgeldajo dobro, burn-in del ni viden.



```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}

# Izluščenje vzorčenj pričakovanih vrednosti za željene šole
mu10  <- cbind(muGroups.all[, 10],  10,  1:n.iter)
mu18  <- cbind(muGroups.all[, 18],  18,  1:n.iter)
mu25  <- cbind(muGroups.all[, 25],  25,  1:n.iter)
mu42 <- cbind(muGroups.all[, 42], 42, 1:n.iter)

# združevanje željenih vzorčenj upanj v skupno tabelo 
mu_data <- data.frame(rbind(mu10, mu18, mu25, mu42))
colnames(mu_data) <- c("mu", "sole", "x")

ggplot(mu_data, aes(x=x, y=mu)) +
    geom_line() +
    facet_wrap(vars(sole), ncol=1) +
    labs(x="st. vzorcenj",
         y=expression(mu[j]),
         title=expression(paste("Trace ploti za ", mu[j], ", j = 10, 18, 25, 42")))
```



```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}

mu10  <- cbind(muGroups.all[1:500, 10],  10,  1:500)
mu18  <- cbind(muGroups.all[1:500, 18],  18,  1:500)
mu25  <- cbind(muGroups.all[1:500, 25],  25,  1:500)
mu42 <- cbind(muGroups.all[1:500, 42], 42, 1:500)

mu_data_500 <- data.frame(rbind(mu10, mu18, mu25, mu42))
colnames(mu_data_500) <- c("mu", "sola", "x")

ggplot(mu_data_500, aes(x=x, y=mu)) +
    geom_line() +
    facet_wrap(vars(sola), ncol=1) +
    labs(x="st. vzorcenj",
         y=expression(mu[j]),
         title=expression(paste("Trace ploti za prvih 500 členov ", mu[j], " j = 10, 18, 25, 42")))
```


```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}

sig10  <- cbind(sigma2Groups.all[, 10],  10,  1:n.iter)
sig18  <- cbind(sigma2Groups.all[, 18],  18,  1:n.iter)
sig25  <- cbind(sigma2Groups.all[, 25],  25,  1:n.iter)
sig42 <- cbind(sigma2Groups.all[, 42], 42, 1:n.iter)

sig_data <- data.frame(rbind(sig10, sig18, sig25, sig42))
colnames(sig_data) <- c("sigma", "sole", "x")

ggplot(sig_data, aes(x=x, y=sigma)) +
    geom_line() +
    facet_wrap(vars(sole), ncol=1) +
    labs(x="st. vzorcenj",
         y=expression(sigma[j]^2),
         title=expression(paste("Trace ploti za ", sigma[j]^2, " j = 10, 18, 25, 42")))
```

```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}

sig10  <- cbind(sigma2Groups.all[1:500, 10],  10,  1:500)
sig18  <- cbind(sigma2Groups.all[1:500, 18],  18,  1:500)
sig25  <- cbind(sigma2Groups.all[1:500, 25],  25,  1:500)
sig42 <- cbind(sigma2Groups.all[1:500, 42], 42, 1:500)

sig_data_500 <- data.frame(rbind(sig10, sig18, sig25, sig42))
colnames(sig_data_500) <- c("sigma", "sola", "x")

ggplot(sig_data_500, aes(x=x, y=sigma)) +
    geom_line() +
    facet_wrap(vars(sola), ncol=2) +
    labs(x="st. vzorcenj",
         y=expression(sigma[j]^2),
         title=expression(paste("Trace ploti za prvih 500 členov", sigma[j]^2,
                                " j = 10, 18, 25, 42")))
```

Konvergence izgledajo vredu, prav tako ni vidne potrebe po kakršnemkoli burn-in parametru. Med šolami se podatki pričakovano nekoliko razlikujejo tako v povprečjih kot v variancah.



## Porazdelitev podvzorcev

```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}


mu_data <- mutate(mu_data, podvzorec=rep(factor(sort(rep(1:10, n.iter/10))), 4))
sig_data <- mutate(sig_data, podvzorec=rep(factor(sort(rep(1:10, n.iter/10))), 4))
hiperparametri <- mutate(hiperparametri, podvzorec=rep(factor(sort(rep(1:10, n.iter/10))), 4))

ggplot(mu_data, aes(x=podvzorec, y=mu)) +
    geom_boxplot() +
    facet_wrap(vars(sole), ncol=2) +
    labs(x="podvzorec",
         y=expression(mu[j]),
         title=expression(paste("Porazdelitve podvzorcev za ", mu[j], " j = 10, 18, 25, 42")))

```


```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}

ggplot(sig_data, aes(x=podvzorec, y=sigma)) +
    geom_boxplot() +
    facet_wrap(vars(sole), ncol=2) +
    labs(x="podvzorec",
         y=expression(sigma[j]^2),
         title=expression(paste("Porazdelitve podvzorcev za ", sigma[j]^2, " j = 10, 18, 25, 42")))
```

```{r, echo=FALSE, eval=TRUE, results="markup", fig.align="center", fig.width=10}
ggplot(hiperparametri, aes(x=podvzorec, y=value)) + 
    geom_boxplot() + 
    facet_wrap(~vars_,
               scales="free_y",
               ncol=2,
               labeller=label_parsed) +
    labs(x="podvzorec",
         y="vrednost parametra",
         title="Porazdelitve podvzorcev")
```
## Avtokorelacije


Po domače je avtokorelacija meritev povezave med trenutno vrednostjo slučajne spremenljivke in preteklimi vrednostmi. V naši nalogi si za njeno računanje pomagamo kar z vgrajeno funkcijo acf.

```{r, echo=F}
head(acf(mu.all)$acf)
head(acf(eta2.all)$acf)
head(acf(sigma20.all)$acf)
head(acf(nu0.all)$acf)
head(acf(muGroups.all[,1])$acf)
head(acf(sigma2Groups.all[,1])$acf)
```

Pri zamiku (*lag*) 1 dobimo po definiciji avtokorelacijo 1, za kasnejse pa si zelimo, da so cim blizje 0 (kar pricakujemo pri n.e.p. vzorcu). Pri vzorcu dobljenim z MCMC metodami bo prisotna avtokorelacija, ki pa se z zamikom (*lag*) zmanjsuje (odvisnost neke vrednosti od vrednosti iz enega koraka nazaj je najvecja, iz dveh korakov nazaj malo manjsa, itd.).

Kako lahko priblizno izracunamo avtokorelacijo z zamikom 1? Ta izračun, analogen 6. nalogi, je ustrezen za parameter $\mu$. S spodnjim izračunom preverimo, da sta dejanska in približno izračunana vrednost (relativno) blizu.

```{r, echo=F}
ac.mu = acf(mu.all, plot = FALSE)
cor(mu.all[-length(mu.all)], mu.all[-1])
ac.mu$acf[2] 
```

Podobne obravnave bo deležen le še hiperparameter $\nu_0$ zaradi podobnosti računanj avtokorelacij z različnimi zamiki. Oglejmo si še zamik za $3$.

```{r, echo=F}
ac.nu0 = acf(nu0.all, plot = FALSE)
cor(nu0.all[-c(length(nu0.all):(length(nu0.all)-2))], nu0.all[-c(1:3)])
ac.nu0$acf[4] 
```

Velja omeniti še obravnavo prvih nekaj členov markovske verige, ki smo jo simulirali s splošnim Gibbsovim vzorčevalnikom. Zamakne se meja na grafih avtokorelacije glede na prejšnji primer.

```{r}
acf(mu.all[1:100]) #za prvih 100 iteracij
```

Kaj predstavljata crti?

$\pm 1.96 / \sqrt{N}$, $N$ stevilo iteracij -- avtokorelacije izven obmocja so statisticno znacilno razlicne od 0 (oz. le 5% jih lahko pricakujemo *nekoliko* izven pri n.e.p. vzorcu).


Kaj se v nasem primeru zgodi z avtokorelacijami, ce v zaporedju izbrisemo vsakega drugega (uporabimo *thinning*)?

Specificiramo dva primera za oba parametra, ki smo ju podrobneje obravnavali že zgoraj: $\nu_0$ in $\mu$.

```{r, echo=F, out.width="120%"}
par(mfrow=c(2,2))
acf(mu.all[seq(1, length(mu.all), by=2)], main="Thinning (vsak drugi) za mu")
acf(mu.all[seq(1, length(mu.all), by=3)], main="Thinning (vsak tretji) za mu")
acf(nu0.all[seq(1, length(nu0.all), by=2)], main="Thinning (vsak drugi) za nu0")
acf(nu0.all[seq(1, length(nu0.all), by=3)], main="Thinning (vsak tretji) za nu0")
```

Pricakovano se zmanjsajo, vendar smo pri tem zmanjsali tudi velikost vzorca, za katerega smo ze porabili cas za izracun. Opazna je tudi razlika med tanjšanjem s faktorjem $2$ in faktorjem $3$.


# Robne aposteriorne porazdelitve

```{r, echo=FALSE, out.width = '110%'}
par(mfrow=c(2,2))

# GOSTOTE HIPERPARAMETROV 
plot(density(filter(hiperparametri, vars_ == "mu")$value), type="l", main="mu")
abline(v = quantile(filter(hiperparametri, vars_ == "mu")$value, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(hiperparametri, vars_ == "eta^2")$value), type="l", main="eta2")
abline(v = quantile(filter(hiperparametri, vars_ == "eta^2")$value, prob=c(0.025, 0.5, 0.975)), lty = 2)

# plot(density(muGroups.all[,1]), type="l", main="mu_1")
# abline(v = quantile(muGroups.all[,1], prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(hiperparametri, vars_ == "sigma[0]^2")$value), type="l", main="sigma2")
abline(v = quantile(filter(hiperparametri, vars_ == "sigma[0]^2")$value, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(hiperparametri, vars_ == "nu[0]")$value), type="l", main="nu0")
abline(v = quantile(filter(hiperparametri, vars_ == "nu[0]")$value, prob=c(0.025, 0.5, 0.975)), lty = 2)

# GOSTOTE UPANJ POSAMEZNIH SOL
plot(density(filter(mu_data, sole ==10)$mu), type="l", main="mu, šole 10")
abline(v = quantile(filter(mu_data, sole ==10)$mu, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(mu_data, sole ==18)$mu), type="l", main="mu, šole 18")
abline(v = quantile(filter(mu_data, sole ==18)$mu, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(mu_data, sole ==25)$mu), type="l", main="mu, šole 25")
abline(v = quantile(filter(mu_data, sole ==25)$mu, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(mu_data, sole ==42)$mu), type="l", main="mu, šole 42")
abline(v = quantile(filter(mu_data, sole ==42)$mu, prob=c(0.025, 0.5, 0.975)), lty = 2)

# GOSTOTE VARIANC POSAMEZNIH SOL

plot(density(filter(sig_data, sole ==10)$sigma), type="l", main="sigma, šole 10")
abline(v = quantile(filter(sig_data, sole ==10)$sigma, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(sig_data, sole ==18)$sigma), type="l", main="sigma, šole 18")
abline(v = quantile(filter(sig_data, sole ==18)$sigma, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(sig_data, sole ==25)$sigma), type="l", main="sigma, šole 25")
abline(v = quantile(filter(sig_data, sole ==25)$sigma, prob=c(0.025, 0.5, 0.975)), lty = 2)

plot(density(filter(sig_data, sole ==42)$sigma), type="l", main="sigma, šole 42")
abline(v = quantile(filter(sig_data, sole ==42)$sigma, prob=c(0.025, 0.5, 0.975)), lty = 2)

```
Oblike vseh gostot se ujemajo z oblikami porazdelitev iz katerih vzorčimo, prav tako vidimo da se porazdelitve upanj in varianc med različnimi šolami razlikujejo, kar nam pove nekaj o primerjavi šol.



# *Shrinkage*

UPANJA:
```{r, echo=F}

plot(density(muGroups.all[,10]), type="l", main="")
points(pod.sole[10,]$povprecje, 0, pch=16, cex=1.5)
abline(v = mean(muGroups.all[,10]), lty=2)
lines(density(muGroups.all[,18]), type="l", col="red")
points(pod.sole[18,]$povprecje, 0, pch=16, cex=1.5, col="red")
abline(v = mean(muGroups.all[,18]), lty=2, col="red")
abline(v = mean(mu.all), lty=2, col="green3")
legend("topleft", c("10. sola", "njeno vz. povp.", "njen E(apost)", 
                    "18. sola", "njeno vz. povp.", "njen E(apost)"), 
       col=c("black","black","black","red","red","red"), lty=c(1,NA,2,1,NA,2), 
       pch=c(NA,16,NA,NA,16,NA))

```


```{r, out.width = '110%', echo=F}
#povprecje

pod.sole$EmuGroups = colMeans(muGroups.all)

par(mfrow=c(1,2))
plot(pod.sole$povprecje, pod.sole$EmuGroups,
     xlab = "vzorcno povprecje", ylab = expression(E(mu_j)))
abline(a = 0, b = 1)

plot(pod.sole$n, pod.sole$povprecje - pod.sole$EmuGroups,
     xlab = "velikost vzorca sole", 
     ylab = expression(paste("vzorcno povprecje - "," ",E(mu_j), sep="")))
abline(h = 0)
```


VARIANCE:

```{r, echo=F}
plot(density(sigma2Groups.all[,10]), type="l", main="")
points(pod.sole[10,]$varianca, 0, pch=16, cex=1.5)
abline(v = mean(sigma2Groups.all[,10]), lty=2)
lines(density(sigma2Groups.all[,18]), type="l", col="red")
points(pod.sole[18,]$varianca, 0, pch=16, cex=1.5, col="red")
abline(v = mean(sigma2Groups.all[,18]), lty=2, col="red")
abline(v = mean(eta2.all), lty=2, col="green3") 
legend("topright", c("10. sola", "njena vz. var.", "njen E(apost)",
                    "18. sola", "njena vz. var..", "njen E(apost)"),
       col=c("black","black","black","red","red","red"), lty=c(1,NA,2,1,NA,2),
       pch=c(NA,16,NA,NA,16,NA))
```

```{r, out.width = '110%', echo=F}
pod.sole$Esigma2Groups = colMeans(sigma2Groups.all)

par(mfrow=c(1,2))
plot(pod.sole$varianca, pod.sole$Esigma2Groups,
     xlab = "vzorcna varianca", ylab = expression(E(sigma2_j)))
abline(a = 0, b = 1)

plot(pod.sole$n, pod.sole$varianca - pod.sole$Esigma2Groups,
     xlab = "velikost vzorca sole", 
     ylab = expression(paste("vzorcna varianca - "," ", E(sigma_j), sep="")))
abline(h = 0)
```


# Primerjava

Rezultati so podobni rezultatom 6. sklopa, je pa vzorčenje z posplosenimi sigmami računsko veliko zahtevneje, zato se splača pretehtati, ce je posplosevanje smiselno.
















